{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.nn import DataParallel\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, AutoModelForCausalLM, BertConfig, BertForPreTraining, BertForMaskedLM, AutoConfig, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import json\n",
    "import tqdm\n",
    "import sys\n",
    "import pyfaidx\n",
    "sys.path.append(\"../src/regulatory_lm/\")\n",
    "from evals.nucleotide_dependency import *\n",
    "from modeling.model import *\n",
    "from utils.viz_sequence import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str_dict = MODULES\n",
    "FLOAT_DTYPES = {\"float32\":torch.float32, \"float64\":torch.float64, \"bfloat16\":torch.bfloat16, \"float16\":torch.float16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    \n",
    "def encode_sequence(sequence): \n",
    "    encoded_sequence = [MAPPING.get(nucleotide, 4) for nucleotide in sequence]\n",
    "    return encoded_sequence\n",
    "\n",
    "def revcomp(seq_list):\n",
    "    return [3-x for x in seq_list][::-1]\n",
    "\n",
    "def revcomp_string(dna_sequence):\n",
    "    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}\n",
    "    return ''.join(complement[base] for base in reversed(dna_sequence.upper()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = \"/oak/stanford/groups/akundaje/patelas/regulatory_lm/data/hg38_repeat_lowercase.fa\" #Replace with your path\n",
    "genome_data = pyfaidx.Fasta(genome, sequence_always_upper=True)\n",
    "#Size does not matter - we analyze a stretch of size seq_len centered around the provided location\n",
    "chrom = \"chr3\"\n",
    "seq_len = 350\n",
    "start = 4868352\n",
    "end = 4868665\n",
    "midpoint = (start + end) // 2\n",
    "start = midpoint - seq_len // 2\n",
    "end = midpoint + seq_len // 2\n",
    "print(midpoint, start, end)\n",
    "dna_seq = genome_data[chrom][start:end].seq\n",
    "seq_tensor = torch.tensor(encode_sequence(dna_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyenaDNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"LongSafari/hyenadna-large-1m-seqlen-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"right\")\n",
    "model =  AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tensor = tokenizer([dna_seq], return_tensors = 'pt')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, demarcs=[], limits=None):\n",
    "    plt.figure(dpi=300, figsize=[16,8])\n",
    "    plot_weights(scores)\n",
    "    plt.xticks([])\n",
    "    max_val = np.abs(scores).max()\n",
    "    if max_val < 0.05:\n",
    "        plt.ylim(-0.05, 0.05)\n",
    "    for motif in demarcs:\n",
    "        plt.axvline(motif[0], color=\"black\")\n",
    "        plt.axvline(motif[1], color=\"black\")\n",
    "    if limits is not None:\n",
    "        plt.ylim(limits[0], limits[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq, mask_inds, model, tokenizer, demarcs=[], limits=None):\n",
    "    '''\n",
    "    Performs inference over a sequence using HyenaDNA\n",
    "    Since it's an autoregressive model, there's no masking\n",
    "    We start the sequence off with a \"C\" to begin the likelihood calculations\n",
    "    '''\n",
    "    one_hot = torch.zeros(1, len(seq), 4, dtype=torch.int8)\n",
    "    seq_encoded_true = encode_sequence(seq)\n",
    "    seq_tensor_true = torch.tensor(seq_encoded_true)\n",
    "    for nuc in range(4):\n",
    "        one_hot[:,:,nuc] = (seq_tensor_true == nuc).to(dtype=torch.int8) # for non ACGT, set to 0\n",
    "    one_hot = one_hot.cpu().numpy(force=True).transpose([0,2,1])\n",
    "    seq_tensor = tokenizer([seq], return_tensors = 'pt')[\"input_ids\"]\n",
    "    seed_token = torch.tensor([[8]]) #Need something to start off with bc it's autoregressive\n",
    "    seq_tensor = torch.cat([seed_token, seq_tensor], dim=1)\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    with torch.no_grad():\n",
    "        seq_tensor = seq_tensor.to(device)\n",
    "        model = model.to(device)\n",
    "        logits = model(seq_tensor).logits\n",
    "        probs_norm = softmax(logits)[:,:-2,7:11] #In addition to the first C, there's also an EOS token\n",
    "    nuc_average = torch.mean(probs_norm, dim=1)\n",
    "    probs_norm = (probs_norm * torch.log(probs_norm / nuc_average)).permute(0,2,1).cpu().numpy()\n",
    "    plot_scores(probs_norm[:,:,min(mask_inds):max(mask_inds) + 1] * one_hot[:,:,min(mask_inds):max(mask_inds) + 1], demarcs, limits)\n",
    "    plot_scores(probs_norm[:,:,min(mask_inds):max(mask_inds) + 1], demarcs, limits)\n",
    "    return probs_norm * one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_norm = predict(dna_seq, list(range(0, 350)), model, tokenizer, limits=[0,1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caduceus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"kuleshov-group/caduceus-ps_seqlen-131k_d_model-256_n_layer-16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"right\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tensor = tokenizer([dna_seq], return_tensors = 'pt')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, demarcs=[], limits=None):\n",
    "    plt.figure(dpi=300, figsize=[16,8])\n",
    "    plot_weights(scores)\n",
    "    plt.xticks([])\n",
    "    max_val = np.abs(scores).max()\n",
    "    if max_val < 0.05:\n",
    "        plt.ylim(-0.05, 0.05)\n",
    "    for motif in demarcs:\n",
    "        plt.axvline(motif[0], color=\"black\")\n",
    "        plt.axvline(motif[1], color=\"black\")\n",
    "    if limits is not None:\n",
    "        plt.ylim(limits[0], limits[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq, mask_inds, model, tokenizer, demarcs=[], limits=None):\n",
    "    new_probs = []\n",
    "    one_hot = torch.zeros(1, len(seq), 4, dtype=torch.int8)\n",
    "    seq_encoded_true = encode_sequence(seq)\n",
    "    seq_tensor_true = torch.tensor(seq_encoded_true)\n",
    "    for nuc in range(4):\n",
    "        one_hot[:,:,nuc] = (seq_tensor_true == nuc).to(dtype=torch.int8) # for non ACGT, set to 0\n",
    "    one_hot = one_hot.cpu().numpy(force=True).transpose([0,2,1])\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    for ind in mask_inds:\n",
    "        seq_tensor = tokenizer([seq], return_tensors = 'pt')[\"input_ids\"]\n",
    "        seq_tensor[:,ind] = 3\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model = model.to(device)\n",
    "            logits = model(seq_tensor.to(device)).logits\n",
    "            probs = F.softmax(logits, dim=-1)[:,:-1,7:11]\n",
    "            probs_norm = probs.cpu().permute(0,2,1)\n",
    "            new_probs.append(probs_norm[:,:,ind])\n",
    "    for i, ind in enumerate(mask_inds):\n",
    "        probs_norm[:,:,ind] = new_probs[i]\n",
    "    probs_norm = probs_norm.permute(0,2,1)\n",
    "    nuc_average = torch.mean(probs_norm, dim=1)\n",
    "    probs_norm = (probs_norm * torch.log(probs_norm / nuc_average)).permute(0,2,1).cpu().numpy()\n",
    "    plot_scores(probs_norm[:,:,min(mask_inds):max(mask_inds) + 1] * one_hot[:,:,min(mask_inds):max(mask_inds) + 1], demarcs, limits)\n",
    "    plot_scores(probs_norm[:,:,min(mask_inds):max(mask_inds) + 1], demarcs, limits)\n",
    "    return probs_norm * one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_norm = predict(dna_seq, list(range(0, 350)), model, tokenizer, limits=[0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LanguageModelingEnv",
   "language": "python",
   "name": "languagemodelingenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
