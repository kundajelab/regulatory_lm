{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to specify the path to arsenal_chrombpnet repository\n",
    "arsenal_chrombpnet_path = \"/users/patelas/arsenal-chrombpnet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.nn import DataParallel\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import tqdm\n",
    "import sys\n",
    "import pyfaidx\n",
    "sys.path.append(\"../src/regulatory_lm/\")\n",
    "from evals.nucleotide_dependency import *\n",
    "from modeling.model import *\n",
    "from utils.viz_sequence import *\n",
    "sys.path.append(f\"{arsenal_chrombpnet_path}/chrombpnet/\")\n",
    "from bpnet import BPNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use ARSENAL as part of a guided sequence generation pipeline. This pipeline employs a beam search strategy, with several rounds of generation. After each round, we score our generated sequences and keep the top $k$ according to a user-defined objective function. \n",
    "\n",
    "The notebook is optimized to use the supervised model ChromBPNet as an oracle to score generations, as in the examples in the ARSENAL paper. However, this is not necessary. Any other model - or no model - can be used by defining the appropriate objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dna_to_one_hot(seqs):\n",
    "    \"\"\"\n",
    "    Converts a list of DNA (\"ACGT\") sequences to one-hot encodings, where the\n",
    "    position of 1s is ordered alphabetically by \"ACGT\". `seqs` must be a list\n",
    "    of N strings, where every string is the same length L. Returns an N x L x 4\n",
    "    Pytorch tensor of one-hot encodings, in the same order as the input sequences.\n",
    "    All bases will be converted to upper-case prior to performing the encoding.\n",
    "    Any bases that are not \"ACGT\" will be given an encoding of all 0s.\n",
    "    \"\"\"\n",
    "    seq_len = len(seqs[0])\n",
    "    assert np.all(np.array([len(s) for s in seqs]) == seq_len)\n",
    "\n",
    "    # Join all sequences together into one long string, all uppercase\n",
    "    seq_concat = \"\".join(seqs).upper() + \"ACGT\"\n",
    "    # Add one example of each base, so np.unique doesn't miss indices later\n",
    "\n",
    "    one_hot_map = np.identity(5)[:, :-1].astype(np.int8)\n",
    "\n",
    "    # Convert string into array of ASCII character codes;\n",
    "    base_vals = np.frombuffer(bytearray(seq_concat, \"utf8\"), dtype=np.int8)\n",
    "\n",
    "    # Anything that's not an A, C, G, or T gets assigned a higher code\n",
    "    base_vals[~np.isin(base_vals, np.array([65, 67, 71, 84]))] = 85\n",
    "\n",
    "    # Convert the codes into indices in [0, 4], in ascending order by code\n",
    "    _, base_inds = np.unique(base_vals, return_inverse=True)\n",
    "\n",
    "    # Get the one-hot encoding for those indices, and reshape back to separate\n",
    "    return torch.tensor(one_hot_map[base_inds[:-4]].reshape((len(seqs), seq_len, 4))).float()\n",
    "\n",
    "\n",
    "model_str_dict = MODULES\n",
    "FLOAT_DTYPES = {\"float32\":torch.float32, \"float64\":torch.float64, \"bfloat16\":torch.bfloat16, \"float16\":torch.float16}\n",
    "MAPPING = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "REVERSE_MAPPING = {0: \"A\", 1: \"C\", 2: \"G\", 3: \"T\"}\n",
    "\n",
    "def encode_sequence(sequence): \n",
    "    encoded_sequence = [MAPPING.get(nucleotide, 4) for nucleotide in sequence]\n",
    "    return encoded_sequence\n",
    "\n",
    "def encode_sequence_tensor(sequence, device):\n",
    "    return torch.tensor(encode_sequence(sequence)).to(device)\n",
    "\n",
    "def revcomp(seq_list):\n",
    "    return [3-x for x in seq_list][::-1]\n",
    "\n",
    "def revcomp_string(dna_sequence):\n",
    "    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}\n",
    "    return ''.join(complement[base] for base in reversed(dna_sequence.upper()))\n",
    "\n",
    "def decode_seq_tensor(seq_tensor):\n",
    "    return \"\".join(REVERSE_MAPPING[x] for x in seq_tensor.tolist())\n",
    "\n",
    "\n",
    "def load_model(args_json, saved_model_file):\n",
    "    args = json.load(open(args_json, \"r\"))\n",
    "    embedder_kwargs = args.get(\"embedder_kwargs\", {})\n",
    "    encoder_kwargs = args.get(\"encoder_kwargs\", {})\n",
    "    decoder_kwargs = args.get(\"decoder_kwargs\", {})\n",
    "    model_kwargs = args.get(\"model_kwargs\", {})\n",
    "\n",
    "\n",
    "    embedder = model_str_dict[args[\"embedder\"]](args[\"embedding_size\"], vocab_size=args[\"num_real_tokens\"]+2, masking=True, **embedder_kwargs)\n",
    "    encoder = model_str_dict[args[\"encoder\"]](args[\"embedding_size\"], args[\"num_encoder_layers\"], **encoder_kwargs)\n",
    "    decoder = model_str_dict[args[\"decoder\"]](args[\"embedding_size\"], **decoder_kwargs)\n",
    "    model = RegulatoryLM(embedder, encoder, decoder)\n",
    "    model_info = torch.load(saved_model_file)\n",
    "    if list(model_info[\"model_state\"].keys())[0][:7] == \"module.\":\n",
    "        model_info[\"model_state\"] = {x[7:]:model_info[\"model_state\"][x] for x in model_info[\"model_state\"]}\n",
    "    else:\n",
    "        model = torch.compile(model)\n",
    "    model.load_state_dict(model_info[\"model_state\"])\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the generation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_generation` is the function that runs the overall targeted generation algorithm. Please see the ARSENAL paper for a detailed description of the algorithm. Here are some key parameters:\n",
    "\n",
    "* `arsenal_model` - ARSENAL model to use\n",
    "* `supervised_models` - list of supervised model(s) to use for scoring generations. Can be an empty argument if you don't want to use any supervised models; just needs to be compatible with the rest of the pipeline\n",
    "* `starting_seq` - starting sequence for the algorithm. Can be any length, and the algorithm will edit the central 350bp\n",
    "* `count_target` - target values to compare with the scores you produce. Can be any data type or length; just needs to be compatible with the scores and objective function\n",
    "* `seqs_to_gen` - number of sequences to generate (and keep after every step in our beam search)\n",
    "* `iters_per_seq` - number of iterations of masking and resampling in each generation step\n",
    "* `mask_ratio` - fraction of tokens to mask\n",
    "* `temp` - temperature for scaling of logits before resampling\n",
    "\n",
    "\n",
    "The other key function to edit is `supervised_predict_counts`. This function takes in a sequence (potentially with an edited center) and uses a supervised model to predict a particular value to eventually compare with one of the target values. The current sequence works for a ChromBPNet model, but if another model is desired, then it will have to be changed. If the user desires a value that does not involve any model predictions, then the function can take a dummy argument for the model and define the calculations accordingly. Note that this function is run separately for every supervised model specified. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seqs_and_context(start_seq, extract_len):\n",
    "    '''\n",
    "    Given a sequence as a string, extracts the center \"extract_len\" positions and separates those from the rest\n",
    "    '''\n",
    "    start_len = len(start_seq)\n",
    "    center_start, center_end = start_len // 2 - extract_len // 2, start_len // 2 + extract_len // 2\n",
    "    center_seq = start_seq[center_start : center_end]\n",
    "    left, right = start_seq[:center_start], start_seq[center_end:]\n",
    "    return center_seq, left, right\n",
    "\n",
    "def filter_best(seqs_and_errors, num_to_keep):\n",
    "    '''\n",
    "    Expects seqs_and_scores to contain tuple of (seq, score) where lower is better\n",
    "    Takes the top k according to num_to_keep\n",
    "    '''\n",
    "    return sorted(seqs_and_errors, key=lambda x: x[1])[:num_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the functions for actual sequence generation\n",
    "def mask_and_resample(model, seq, mask_token=5, mask_ratio=0.15, temp=1.0):\n",
    "    # Step 1: Generate mask\n",
    "    to_mask = torch.rand(seq.shape, device=seq.device) < mask_ratio\n",
    "    # Step 2: Mask sequence\n",
    "    masked_seq = torch.where(to_mask, mask_token, seq)\n",
    "    # Step 3: Forward pass through model\n",
    "    with torch.no_grad():\n",
    "        logits = model(masked_seq.unsqueeze(0), None) / temp  # shape: (1, N, V)\n",
    "    # Step 4: Get probabilities\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)  # shape: (N, V)\n",
    "    # Step 5: Sample for *masked positions only*\n",
    "    sampled_indices = torch.multinomial(probs[to_mask], num_samples=1).squeeze(1)  # shape: (num_masked,)\n",
    "\n",
    "    # Step 6: Create output by replacing masked positions with sampled values\n",
    "    out = seq.clone()\n",
    "    out[to_mask] = sampled_indices\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def iterative_generation(model, seq, iters=100, mask_token=5, mask_ratio=0.15, temp=1.0):\n",
    "    for gen_run in range(iters):\n",
    "        seq = mask_and_resample(model, seq, mask_token, mask_ratio, temp)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the functions to perform and evaluate generations\n",
    "def generate_and_predict(arsenal_model, supervised_models, seq, context, iters_per_seq, mask_token, mask_ratio, temp, device):\n",
    "    '''\n",
    "    Given a starting sequence, generates a new sequence from it\n",
    "    Combines the sequences with the surrounding left and right context and uses supervised model to predict counts\n",
    "    Returns the generated central sequence and counts\n",
    "    '''\n",
    "    seq_tensor = encode_sequence_tensor(seq, device)\n",
    "    new_seq = iterative_generation(arsenal_model, seq_tensor, iters_per_seq, mask_token, mask_ratio, temp)\n",
    "    new_seq_str = decode_seq_tensor(new_seq)\n",
    "    supervised_full_seq = context[0] + new_seq_str + context[1]\n",
    "    pred_counts = [supervised_predict_counts(sup_model, supervised_full_seq, device) for sup_model in supervised_models]\n",
    "    return new_seq_str, pred_counts\n",
    "    \n",
    "def supervised_predict_counts(supervised_model, seq_str, device):\n",
    "    '''\n",
    "    Takes in a DNA sequence as a string and uses a supervised bpnet-style model to predict counts over the region\n",
    "    '''\n",
    "    one_hot_seq = dna_to_one_hot([seq_str]).to(device)\n",
    "    with torch.no_grad():\n",
    "        supervised_pred = supervised_model(one_hot_seq)\n",
    "    return supervised_pred[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the full target generation loop\n",
    "def target_generation(arsenal_model, supervised_models, starting_seq, count_target, device, error_fn, seqs_to_gen=100, gen_iters=50, len_to_edit=350, iters_per_seq=100,\n",
    "                                mask_token=5, mask_ratio=0.15, temp=1.0):\n",
    "    '''\n",
    "    Performs the full ChromBPNet-aided ARSENAL generation pipeline. Works as follows:\n",
    "    1. Extract central region from starting input sequence - this is what we will directly edit\n",
    "    2. Generate one sequence and predict using one or more BPNet-style supervised models, add to our list\n",
    "    3. Now, in each iteration, we add one new sequence per each sequence already in the list\n",
    "    4. At the end of each iteration, we filter to only the top k sequences according to our error metric (if more than that exist at the time)\n",
    "    5. At the end, we return our final top k\n",
    "    \n",
    "    Note that supervised_models and count_target must be iterables\n",
    "    '''\n",
    "    seqs_and_errors = [] #Each element will be a tuple of (seq, error)\n",
    "    edit_seq, left_context, right_context = extract_seqs_and_context(starting_seq, len_to_edit)\n",
    "    \n",
    "    #Generate first sequence so we have something to populate\n",
    "    first_seq, first_counts = generate_and_predict(arsenal_model, supervised_models, edit_seq, (left_context, right_context), iters_per_seq, mask_token, mask_ratio, temp, device)\n",
    "    error = error_fn(count_target, first_counts)\n",
    "    seqs_and_errors.append((first_seq, error))\n",
    "    \n",
    "    #Now go through our iterations until we're done\n",
    "    #For each generated sequence we currently have, we will generate a new one from it and keep the top n\n",
    "    for gen_run in range(gen_iters):\n",
    "        print(\"Iteration \", gen_run)\n",
    "        curr_iter_list = seqs_and_errors.copy()\n",
    "        for (gen_seq, error) in curr_iter_list:\n",
    "            new_seq, new_counts = generate_and_predict(arsenal_model, supervised_models, gen_seq, (left_context, right_context), iters_per_seq, mask_token, mask_ratio, temp, device)\n",
    "            seqs_and_errors.append((new_seq, error_fn(count_target, new_counts)))\n",
    "        \n",
    "        #At the end of each iteration, we will keep only the best sequences for the next iteration\n",
    "        seqs_and_errors = filter_best(seqs_and_errors, seqs_to_gen)\n",
    "        \n",
    "    return seqs_and_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some useful objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are simple objective functions which are useful for scoring generations and form an essential part of the pipeline. \n",
    "\n",
    "Feel free to define your own functions. The only constraint is that they must take in two arguments: target values (`true_counts`) and some values produced from the generations (`predicted_counts`; the outputs of the calls to the `supervised_predict_counts` function) and must return a scalar score. How each of the inputs is used to produce the score (if at all) is up to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_error(true_counts, predicted_counts):\n",
    "    return sum([abs(true_counts[x] - predicted_counts[x]) for x in range(len(true_counts))])\n",
    "\n",
    "def squared_dist(true_counts, predicted_counts):\n",
    "    true_counts, predicted_counts = np.array(true_counts), np.array(predicted_counts)\n",
    "    return np.linalg.norm(true_counts - predicted_counts)\n",
    "\n",
    "\n",
    "def max_diff(true_counts, predicted_counts):\n",
    "    '''\n",
    "    This only works with two models, maximizes the first one with respect to the second one\n",
    "    (Lower score is better so we switch the subtraction)\n",
    "    '''\n",
    "    return predicted_counts[1] - predicted_counts[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: ChromBPNet-guided generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we show an example of ChromBPNet-guided generation from the paper. We want to generate sequences which have high predicted counts in the HEPG2 cell line and low predicted counts in the H1-hESC cell line. We will use targets of 8 counts in the first and 1 count in the second, utilizing the absolute error objective function we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ARSENAL Model\n",
    "args_json = \"/mnt/lab_data2/regulatory_lm/scratch/transformer_test/run_20251231_230449/args.json\" #Model args file\n",
    "saved_model_file = \"/mnt/lab_data2/regulatory_lm/scratch/transformer_test/run_20251231_230449/checkpoint_149.pt\" #Model checkpoint file\n",
    "\n",
    "\n",
    "model = load_model(args_json, saved_model_file).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ChromBPNet Models\n",
    "hepg2_model_file = \"/oak/stanford/groups/akundaje/projects/chromatin-atlas-2022/DNASE/ENCSR149XIL/chrombpnet_model/chrombpnet_wo_bias.h5\" #Substitute path here\n",
    "hepg2_chrombpnet_model = BPNet.from_keras(hepg2_model_file)\n",
    "hepg2_chrombpnet_model = hepg2_chrombpnet_model.to(device)\n",
    "\n",
    "h1esc_model_file = \"/oak/stanford/groups/akundaje/projects/chromatin-atlas-2022/DNASE/ENCSR000EMU/chrombpnet_model/chrombpnet_wo_bias.h5\" #Substitute path here\n",
    "h1esc_chrombpnet_model = BPNet.from_keras(h1esc_model_file)\n",
    "h1esc_chrombpnet_model = h1esc_chrombpnet_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define genome and location - we will optimize the 2,114 bp sequence centered on this location\n",
    "genome = \"/mnt/lab_data2/regulatory_lm/oak_backup/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "genome_data = pyfaidx.Fasta(genome, sequence_always_upper=True)\n",
    "chrom = \"chr4\"\n",
    "seq_len = 2114\n",
    "start = 39469376\n",
    "end = 39469725\n",
    "midpoint = (start + end) // 2\n",
    "start = midpoint - seq_len // 2\n",
    "end = midpoint + seq_len // 2\n",
    "dna_seq = genome_data[chrom][start:end].seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We now produce one set of 100 generations - we did 5 such runs for the paper\n",
    "#The function only returns the central 350bp (which is the only part that is edited), but this can easily be combined with the rest\n",
    "#For the cell-type specific generations, we used targets of [8.0,1.0] and [3.0,6.0] for [HEPG2, H1ESC] and a temperature of 1.0\n",
    "#For the HEPG2 counts targeting, we used targets of 5.0, 6.0, 7.0, and 8.0, with a temperature of 0.3\n",
    "new_gen_seqs = target_generation(model, [hepg2_chrombpnet_model, h1esc_chrombpnet_model], dna_seq, [8.0, 1.0], device, absolute_error, gen_iters=40, iters_per_seq=20, mask_ratio=0.01, temp=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LanguageModelingEnv",
   "language": "python",
   "name": "languagemodelingenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
